---
title: Analysis of clustered data part 1.  Use of simulation to study power and error rates of statistical tests.
author: George Savva
date: 2020-02-09
output:
  distill::distill_article:
    self_contained: false
---

```{r global_options, include=TRUE, echo=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE)
```


## Summary

Data arising from biological studies are often clustered, that is, owing to the study design some groups of observations will be more similar to each other than they are to other observations.  

For example, study participants might be sampled in family groups, or mice housed together in cages.  In both of these cases it is possible that there is some factor that influences all of the members of one cluster (that is all the members of a family or mice in one particular cage) but does not affect members of other clusters. This causes what is called an intra-class correlation within clusters. Crucially, this violates the assumption of independence of observations that is required for many of our usual statistical procedures, including t-tests and simple ANOVAs.

In these three posts I explore to what extent clustering affects the accuracy of these statistical tests. I use the t-test as a simple example, but the conclusions are equally applicable to most common statistical procedures.  I wrote these posts  because I very often see simple but incorrect analysis applied to clustered data, and I wanted to know how often this might lead to false positives in experimental results, and how different experimental design might mitigate the problem.

First, I introduce a simulation study to show that the t-test 'works' under usual circumstances, by estimating its power and Type I error probability when the assumptions underlying the test are true.  

In part 2 I show that introducing an intra-class correlation within clusters, thereby violating the assumption of independence of observations, can lead to a very high false  Type 1 error rate. 

Finally, in part 3 I demonstrate two alternative analyses that are valid when this assumption is not met.  Finally I explore the implications for clustering on power, using a common mouse study design as an example.

The key messages are: 

* Observations from units that are not independent must not be analysed as if they are independent.
* Ignoring this can severely inflate Type 1 error rates (leading to false positives). 
* A correct analysis, such as a linear mixed model or simply averaging data over clusters before analysis, is often not very difficult.
* For a mouse experiment: co-housing your mice means that you will need more mice than you would if you'd been able to allocate them completely randomly and house them individually, with the size of this effext depending on the specific experimental design and how much intra-class correlation there is likely to be.
* So this must be considered in your design and power calculation

This topic is closely related to the question of what counts as a replicate or a pseudo-replicate, as discussed by.  I prefer a different approach, which is not to think in terms of 'replication' but in terms of the structure of the relationships between different observations.  This is more generalisable since we don't always have a neat nested structure of replication, and its not always straightforward what should be considered as, say, a biological or a technical replicate in an experiment.

## Exploring the properties of statistical tests by Monte Carlo simulation 

Simulation studies are an extremely powerful way to test statistical procedures.  In short, they work by generating thousands of 'fake' datasets with known parameters, and on each dataset checking how well our analysis recovers those parameters.

In these posts I’m going to test the robustness t-test by simulation. My ultimate aim is to understand how well it works when I violate the ‘assumptions’ that accompany it, in particular the assumption of independence of observations, and to know how well I can compensate with better analysis methods.

There are three parts to this exercise.

1. In this post I'll set up a simulation of a t-test working properly, and use the simulation to estimate its power and Type-I error rate.
2. In the second part I'll simulate clustered data, to measure how much clustering inflates the error rate of the t-test under different circumstances.
3. Finally I'll test two supposedly correct approaches to analysing data like this, to check if the error rates are correct and then how powerful they are.

## Motivating example

The motivation for this example is the analysis of data that comes from animal studies, particularly studies of mice, where individual mice are co-housed in cages of say 3-5 animals, and treatment groups might include 2 or 3 cages.  Such studies are extremely common.

It is well known that if treatments are allocated per cage rather than per mouse, then the experimental unit is the *cage* and data should be analysed on that basis.  But in practice this is rarely done, statistical analyses are typically conducted with each mouse considered as an independent experimental unit.  Theoretically we know this is wrong, but how bad are the consequences in practice?

## Simulation study

I’m going to simulate a mouse experiment looking at the effect of a treatment (compared to control) on mouse weight (measured once at the end of the study), with two groups of ten mice per group.  For this first experiment each mouse is housed and treated independently in its own cage, and the mice are randomly allocated to treatments.

Error variance (the variation between mice) will be normally distributed with a standard deviation of 10g in both groups.

In some cases I will simulate data with a real difference of 10g between the groups.  In other cases I will simulate data with no real difference between the groups.  In each case I will be interested in the number of datasets for which a t-test (or other method) returns a statistically significant difference between the groups.

If there is a real difference between the group means, then a 'significant' t-test is a good thing (a true positive), and the proportion of datasets for which the test is statisticially significant estimates the power of the test.

In the cases where there is no real difference, any 'significant' t-test represents a type-1 error (false positive), and so the proportion of these estimates the type-1 error rate of the test.


``` {r chunk1}
set.seed(06102019) # Setting the seed .
simData1 <- function(N=10, b=10, sd=10, a=100){  # 'a' is the control mean, it doesn't matter at all
  treatment <- rep(c(0,1), each=N) #  'treatment' is the treatment group
  weight <- rnorm(2*N, a+b*treatment, sd) #  'weight' is the response
  data.frame(treatment,weight)
}

data1 <- simData1()

```

First lets plot a single run of this experiment, to see how the data look:

``` {r echo=FALSE}
library(lmerTest)
library(ggplot2)
```


``` {r }
ggplot(data=data1, aes(x=factor(treatment), y=weight)) + 
  geom_boxplot(fill="red") + 
  geom_point(size=3) + 
  theme_bw() + labs(x="Group", y="Weight (g)")
```
  
There are two groups of mouse weights, both normally distributed around its group mean.  We can run a t-test now, to test whether the difference between these groups is statistically significant:
 
``` {r }

t.test(data=data1, weight~treatment)

```

So in this case the t-test has correctly identified that there is a significant difference between the two groups.

Let's run another simulation to look at how the results might vary from run to run:

``` {r }

data2 <- simData1()

ggplot(data=data2, aes(x=factor(treatment), y=weight)) + 
  geom_boxplot(fill="red") + geom_point(size=3) + 
  theme_bw() + xlab("Treatment") + ylab("Weight (g)")
t.test(data=data2, weight~treatment)


```

## Power and type-1 error

So from our two exeriments so far the t-test does detect the difference between groups, but how well would we expect it to work on average?


``` {r }
simPvalue <- function(N=10, b=10, sd=10){
  dat <- simData1(N=N, b=b, sd=sd) # get the data
  ttest1 <- t.test(data=dat, weight~treatment) # do the t-test
  ttest1$p.value # return the p-value
}

``` 

Next I ran the experiment 10000 times with a real difference between groups, then 10000 without a real difference between groups.

``` {r }

repsH1_p <- replicate(10000, simPvalue(b=10) )
repsH0_p <- replicate(10000, simPvalue(b=0) )

```

If we plot a histogram of p-values under both hypothesis, we see:

``` {r  }

par(mfrow=c(1,2))

hist(repsH1_p, col="red", breaks=20, main="b=10", xlab="P-value ", ylab="Frequency", freq=TRUE)

hist(repsH0_p, col="blue", breaks=20, main="b=0", xlab="P-value", ylab="Frequency", freq=TRUE)
abline(a=500,b=0)


```

When there is a true difference of 10g between groups then our null hypothesis would be rejected in `r sum(repsH1_p<0.05)` of the 10000 simulations, or about `r round(sum(repsH1_p<0.05)/10000,2)` of the time.

In other words this experimental design has a 56% chance of detecting a statistically significant effect, if the true effect size is b=10, the standard deviation of weights within groups is 10, and there are 10 mice per group.

So using this simulation study we have estimated the power of the test in this situation.

As an aside - this was such a simple case that we could have used the built-in R function to calculate the power directly, as follows:

``` {r, echo=TRUE}

power.t.test( d=10, sd=10, n=10, power=NULL)

```

We can also ask in how many simulations under H0 (no effect) we got a 'significant' result.

``` {r, echo=TRUE }

sum(repsH0_p<0.05)

```

So in `r sum(repsH0_p<0.05)` of the 10000 studies, or in around 5% of the time, there is a false positive result at p<0.05.  This is reassuring, as is the uniform distribution of the p-values under the null hypothesis. (This is exactly what a p-value distribution should look like under a null hypothesis)  It means that the t-test is likely to give us false positives at the rate we chosen when we set our nominal threshold for statistical significance.

## Part 1 conclusion

Here we have shown the t-test 'works' as expected for hypothesis testing in this one simple case, and we have empirically calculated its power with N=10 in both groups for detecting an effect size of 10g and standard deviation of 10g.

In the next post I will instead simulate clustered data, to look at how the type 1 error is affected.  Then we'll look at some alternative analyses that should be valid using these datasets and compare the power and type-1 error of these.