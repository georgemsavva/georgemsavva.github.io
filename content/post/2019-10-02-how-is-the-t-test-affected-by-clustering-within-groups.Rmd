---
title: Analysis of clustered data part 2. How is the t-test affected by clustering
  within groups?
author: "George Savva"
date: '2020-02-10'

---

```{r global_options, include=TRUE, echo=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE)
```

In the last entry we estimated the power and type-1 error rate of the t-test for a simple two group experiment, using datasets simulated under conditions that perfectly matched the assumptions of the test.

We showed that the power of our experiment to detect a real difference between groups of 10g was 56%, and that when there was no underlying difference between groups we got a false positive result 5% of the time (defining statistical significance at p<0.05).

In this post we will simulate clustered data, violating one of the key assumptions of the t-test, to test how clustering affects the risk of a false positive.

# Clustering

Many common experimental designs lead to clustered data.  In particuar, animal experiments are almost always clustered.  For example, if we are conducting a mouse experiment then outcomes from mice housed within the same cage are unlikely to be completely independent of each other.  Mice affect their cage mates, or external cage-level factors (such as the position, temparature, noise, lighting, feed etc) might cause a pair of mice in the same cage to respond more similarly than if they were from different cages.  This clustering in cages, which is clearly seen very often in mouse experiments, would be a direct violation of the assumption of independence of observations.

It's also often the case that treatments need to be allocated per cage rather than per mouse, eg if they are delivered in feed, if or if there is some ethical or practical reason why mice with different treatments should not be housed together.

It is accepted that experimental unit in this case is the cage and not the mouse, although most introductory statistics courses and textbooks do not deal with this scenario, considering it an advanced topic despite complex error structures like this being present more often than not in biological experiments.  Consequently we do not tend to see analyses conducted that correctly incorporate this clustering; many animal studies treat co-housed and co-treated mice as independent, or do not include any information on how mice were housed at all.

This is certainly wrong in theory, but how bad is it in practice?  We'll use a simulation study to find out.

# Adding a cage effect to our simulation study:

To adapt our previous simulation we need to make two additions.  First, we need to decide on a cage layout.  Second we need to say how much correlation there is likely to be within the cages.

It is common to have 3 or 4 mice per cage, so we'll split our 10 mice per group into 2 sets of 3 and 1 of 4.

We'll introduce a small amount of covariance between mice from the same cage, such that of our variance of 10^2, 20% comes from the cage effect and 80% from the individual.  That is the intra-class correlation coefficient is 0.2.  (In practice I typically see intra-class correlation coefficients from mouse experiments between 0 and 0.5).

``` {r }
simPvalue2 <- function( b=1, sd=1, icc=0.2, cageN = c(3,3,4)){
  sd1 <- sd*sqrt(1-icc) # get the between group and within group sd
  sd2 <- sd*sqrt(icc)
  
  x <- rep(c(0,1), each=sum(cageN)) #  x is the treatment group
  cageID <- rep(1:(length(cageN)*2), c(cageN, cageN))
  dat <- data.frame(x, cageID)
  cageeffect <- data.frame(cageID=1:(length(cageN)*2), ce=rnorm(length(cageN)*2, 0, sd2))
  dat <- merge(dat, cageeffect)
  
  dat$y <- rnorm(2*sum(cageN), b*dat$x + dat$ce, sd1) #  y is the response
  ttest1 <- t.test(data=dat, y~x) # do the t-test
  ttest1$p.value # return
}

``` 

Now we'll run 10000 reps to check the distribution of the p-value when b=0, that is when data is simulated with no difference between groups.  To do this I created a function called simPvalue2 that simulates an experiment with a specified number of mice per cage, intra-class correlation within cages and true treatment effect.

```{r, echo=TRUE}
reps2 <- replicate( n=10000, simPvalue2(b=0,icc=0.2, cageN=c(3,3,4)))
cat("Number of false positives: ",sum(reps2<0.05))
```

This is worrying - we've gone from 5% of false positives in the last simulation to around 11% false positives in this simulation.

What happens if we use an alternative layout of 2 cages per group and 5 mice per cage?

``` {r, echo=TRUE}

reps3 <- replicate( n=10000, simPvalue2(b=0,icc=0.2,cageN=c(5,5)))
cat("Number of false positives: ",sum(reps3<0.05))

``` 

Now 15% of experiments go wrong at a nominally 5% level.

This is bad because if there was no true effect you'd be seeing a false positive 15% of the time instead of 5%.

What if we use 5 cages of 2 animals each?

``` {r,echo = TRUE }

reps4 <- replicate( n=10000, simPvalue2(b=0,icc=0.2,cageN=c(2,2,2,2,2)))
cat("Number of false positives: ",sum(reps4<0.05))


``` 

So with design we are back to around 7%, which is much better but not perfect.  Looking at the p-value distributions, as expected they are no longer uniform, but they do become more uniform as the mice are distributed among more cages, suggesting that our analysis works better when there are smaller groups.  This makes sense, with smaller groups anything that affects a cage will affect fewer mice.

``` {r }

par(mfrow=c(1,3))
hist(reps3, col="blue", breaks=20, main="Two cages\nof five", xlab="P-value under H0", ylab="Frequency",freq=TRUE, ylim=c(0,1500))
abline(a=500,b=0)
hist(reps2, col="red", breaks=20, main="Two cages of three\nand one of four", xlab="P-value under H0", ylab="Frequency", freq=TRUE, ylim=c(0,1500))
abline(a=500,b=0)
hist(reps4, col="green", breaks=20, main="Five cages\nof two", xlab="P-value under H0", ylab="Frequency",freq=TRUE, ylim=c(0,1500))
abline(a=500,b=0)



```

Finally, as we vary both the amount of within-cage correlation and the cage layout we can check how the type-1 error rate (the proportion of null hypotheses falsely rejected) changes:

``` {r errorrate}

N=1000
errorrate <- function(icc=0.2){
  reps1 <- replicate( n=N, simPvalue2(b=0,icc=icc,cageN=10))
  reps2 <- replicate( n=N, simPvalue2(b=0,icc=icc,cageN=c(5,5)))
  reps3 <- replicate( n=N, simPvalue2(b=0,icc=icc,cageN=c(3,3,4)))
  reps4 <- replicate( n=N, simPvalue2(b=0,icc=icc,cageN=c(2,2,2,2,2)))

c(sum(reps1<0.05) / N,
  sum(reps2<0.05) / N,
  sum(reps3<0.05) / N,
  sum(reps4<0.05) / N)
  }
  
type1 <- as.data.frame(sapply(c(0.1,0.2,0.3,0.4,0.5), FUN=errorrate))
type1 <- as.data.frame(type1)
names(type1) <- c("ICC=0.1", "ICC=0.2","ICC=0.3","ICC=0.4","ICC=0.5")
type1dat <- data.frame(design=c("One cage of 10", "Two cages of 5", "Two cages of 3 and one of 4", "Five cages of 2"),type1)
knitr::kable(type1dat, type="html")

```



We see that the more variance is shared within cages, and the fewer cages the mice are divided between, the higher the type 1 error.

This is potentially very serious.  You are choosing the p-value of 5% which means you and your readers can expect a false positive from your study in 5% of cases where H0 was true (above 30% in not-unrelialistic situations).  But your false positive rate might in fact be much greater than this, potentially leading to many false claims of differences where none exist.  Consider this the next time you are reading an experiment with such a design or analysis.

# Part 2 conclusion

Our usual statistical tests don't work well if our experimental units are clustered by design.  This can lead to a very high risk of a false positive result.  Next we'll consider how to analyse this data correctly and the implications of clustering for statistical power.
